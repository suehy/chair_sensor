{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, roc_auc_score, auc\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from numpy import argmax\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# start with linear classifiers, non-linear ones and eventually the more complex neural nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a single file as a numpy array\n",
    "# def load_file(filepath):\n",
    "#     dataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
    "#     return dataframe.values\n",
    "\n",
    "# load a dataset group, such as train or test\n",
    "# def load_dataset_group(group, prefix=''):\n",
    "#     # load input data\n",
    "#     X = load_file(prefix + group + '/X_'+group+'.txt')\n",
    "#     # load class output\n",
    "#     y = load_file(prefix + group + '/y_'+group+'.txt')\n",
    "#     return X, y\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "# def load_dataset(prefix=''):\n",
    "#     # load all train\n",
    "#     trainX, trainy = load_dataset_group('train', prefix + 'HARDataset/')\n",
    "#     print(trainX.shape, trainy.shape)\n",
    "#     # load all test\n",
    "#     testX, testy = load_dataset_group('test', prefix + 'HARDataset/')\n",
    "#     print(testX.shape, testy.shape)\n",
    "#     # flatten y\n",
    "#     trainy, testy = trainy[:,0], testy[:,0]\n",
    "#     print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
    "#     return trainX, trainy, testX, testy\n",
    "\n",
    "# def load_dataset(prefix=''):\n",
    "#     # load all train\n",
    "#     train = load_dataset_group('train', prefix + 'HARDataset/')\n",
    "#     print(trainX.shape, trainy.shape)\n",
    "#     # load all test\n",
    "#     testX, testy = load_dataset_group('test', prefix + 'HARDataset/')\n",
    "#     print(testX.shape, testy.shape)\n",
    "#     # flatten y\n",
    "#     trainy, testy = trainy[:,0], testy[:,0]\n",
    "#     print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
    "#     return trainX, trainy, testX, testy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '../data/processed/flori5hz.csv'\n",
    "# values = load_file(path)\n",
    "# #X, y = load_dataset_group()\n",
    "# values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/processed/train.csv')\n",
    "\n",
    "test_df = pd.read_csv('../data/processed/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6033, 1)\n",
      "(6033,)\n"
     ]
    }
   ],
   "source": [
    "X_train = train_df.drop(['state', 'name'], axis=1)\n",
    "y_train = pd.DataFrame(train_df['state'])\n",
    "\n",
    "X_test = test_df.drop(['state', 'name'], axis=1)\n",
    "y_test = pd.DataFrame(test_df['state'])\n",
    "\n",
    "print(y_test.shape)\n",
    "print(test_df['state'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict of standard models to evaluate {name:object}\n",
    "def define_models(models=dict()):\n",
    "    # nonlinear models\n",
    "#     models['knn'] = KNeighborsClassifier()\n",
    "#     models['cart'] = DecisionTreeClassifier()\n",
    "#    models['cart'] = { 'clf': DecisionTreeClassifier(), 'params': {}}\n",
    "    #models['svm'] = SVC()\n",
    "#     models['bayes'] = GaussianNB()\n",
    "    # ensemble models\n",
    "#     models['bag'] = BaggingClassifier(n_estimators=100)\n",
    "    models['rf'] = RandomForestClassifier(n_estimators=100)\n",
    "#     models['et'] = ExtraTreesClassifier(n_estimators=100)\n",
    "    models['gbm'] = GradientBoostingClassifier(n_estimators=100)\n",
    "#     models['bag'] = BaggingClassifier()\n",
    "    #models['rf'] = { 'clf': RandomForestClassifier(n_jobs=-1), 'params': {'n_estimators': [100],\n",
    "#                                                                            'bootstrap': [False],\n",
    "#                                                                            'min_samples_split': [10, 20, 40, 50],\n",
    "#                                                                            'max_features': ['log2', 'sqrt','auto', None], \n",
    "#                                                                           'criterion': ['entropy', 'gini']}}\n",
    "#     models['et'] = ExtraTreesClassifier()\n",
    "#     models['gbm'] = GradientBoostingClassifier()\n",
    "    # sgd is sensitive to feature scaling\n",
    "    #models['sgd'] = SGDClassifier()\n",
    "    #models['gp'] = GaussianProcessClassifier()\n",
    "#     models['mlp'] = MLPClassifier()\n",
    "    print('Defined %d models' % len(models))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined 2 models\n"
     ]
    }
   ],
   "source": [
    "# get model list\n",
    "models = define_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a single model\n",
    "def evaluate_model(trainX, trainy, testX, testy, model):\n",
    "    # fit the model\n",
    "    model.fit(trainX, trainy)\n",
    "    # make predictions\n",
    "    yhat = model.predict(testX)\n",
    "    # evaluate predictions\n",
    "    accuracy = balanced_accuracy_score(testy, yhat)\n",
    "    #roc_auc = roc_auc_score(testy, yhat)\n",
    "    \n",
    "    print(classification_report(testy, yhat))\n",
    "    print(confusion_matrix(testy, yhat))\n",
    "    return accuracy * 100.0\n",
    "    #return roc_auc * 100.0\n",
    "\n",
    "# evaluate a dict of models {name:object}, returns {name:score}\n",
    "def evaluate_models(trainX, trainy, testX, testy, models):\n",
    "    results = dict()\n",
    "    for name, model in models.items():\n",
    "        # evaluate the model\n",
    "        results[name] = evaluate_model(trainX, trainy, testX, testy, model)\n",
    "        # show process\n",
    "        print('>%s: %.3f' % (name, results[name]))\n",
    "    return results\n",
    "\n",
    "# print and plot the results\n",
    "def summarize_results(results, maximize=True):\n",
    "    # create a list of (name, mean(scores)) tuples\n",
    "    mean_scores = [(k,v) for k,v in results.items()]\n",
    "    # sort tuples by mean score\n",
    "    mean_scores = sorted(mean_scores, key=lambda x: x[1])\n",
    "    # reverse for descending order (e.g. for accuracy)\n",
    "    if maximize:\n",
    "        mean_scores = list(reversed(mean_scores))\n",
    "    print()\n",
    "    for name, score in mean_scores:\n",
    "        print('Name=%s, Score=%.3f' % (name, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elizabeth/education/chair_sensor/ml_project/venv/lib/python3.5/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93      2566\n",
      "           1       0.87      0.94      0.90      2661\n",
      "           2       0.68      0.63      0.65       395\n",
      "           3       0.89      0.42      0.57       411\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      6033\n",
      "   macro avg       0.84      0.73      0.76      6033\n",
      "weighted avg       0.88      0.88      0.87      6033\n",
      "\n",
      "[[2401   97   47   21]\n",
      " [ 145 2489   27    0]\n",
      " [  49   98  247    1]\n",
      " [  18  179   43  171]]\n",
      ">gbm: 72.811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elizabeth/education/chair_sensor/ml_project/venv/lib/python3.5/site-packages/ipykernel_launcher.py:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94      2566\n",
      "           1       0.88      0.94      0.91      2661\n",
      "           2       0.73      0.67      0.70       395\n",
      "           3       0.80      0.45      0.58       411\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6033\n",
      "   macro avg       0.84      0.75      0.78      6033\n",
      "weighted avg       0.89      0.89      0.89      6033\n",
      "\n",
      "[[2420   71   31   44]\n",
      " [ 119 2507   35    0]\n",
      " [  52   79  263    1]\n",
      " [  12  183   32  184]]\n",
      ">rf: 74.969\n"
     ]
    }
   ],
   "source": [
    "# evaluate models\n",
    "results = evaluate_models(X_train, y_train, X_test, y_test, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Name=rf, Score=68.030\n"
     ]
    }
   ],
   "source": [
    "### summarize results\n",
    "summarize_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">bayes\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'GaussianNB' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-a03c9daba8d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mrun_nested_logo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mevaluate_tuned_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-a03c9daba8d2>\u001b[0m in \u001b[0;36mevaluate_tuned_models\u001b[0;34m(models, scorer)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mrun_nested_logo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'GaussianNB' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import make_scorer, balanced_accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "\n",
    "all_df = train_df.append(test_df)\n",
    "all_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_all = all_df.drop(['state', 'name'], axis=1)\n",
    "y_all = all_df['state']\n",
    "\n",
    "groups = all_df['name']\n",
    "\n",
    "scorer = make_scorer(balanced_accuracy_score)\n",
    "\n",
    "outcomes = []\n",
    "\n",
    "def run_nested_logo(clf, params, scorer):\n",
    "    logo = LeaveOneGroupOut()\n",
    "    group = 0\n",
    "    \n",
    "    for train_index, test_index in logo.split(X_all, groups=groups):\n",
    "        group += 1\n",
    "        X_train, X_test = X_all.iloc[train_index], X_all.iloc[test_index]\n",
    "        y_train, y_test = y_all.iloc[train_index], y_all.iloc[test_index]\n",
    "        \n",
    "        inner_groups = all_df.iloc[all_df.index.isin(train_index)]['name']\n",
    "\n",
    "        logo = LeaveOneGroupOut()\n",
    "        grid_obj = GridSearchCV(clf, params, scoring=scorer, cv=logo.split(X_train, groups=inner_groups), n_jobs=-1)\n",
    "        grid_obj = grid_obj.fit(X_train, y_train)\n",
    "        \n",
    "        print(grid_obj.best_estimator_)\n",
    "        clf = grid_obj.best_estimator_\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        predictions = clf.predict(X_test)\n",
    "        \n",
    "        accuracy = balanced_accuracy_score(y_test, predictions)\n",
    "        outcomes.append(accuracy)\n",
    "        print(\"Group {0} accuracy: {1}\".format(group, accuracy))\n",
    "    mean_outcome = np.mean(outcomes)\n",
    "    print(\"Mean Accuracy: {0}\".format(mean_outcome))\n",
    "    \n",
    "def evaluate_tuned_models(models, scorer):\n",
    "    results = dict()\n",
    "    for name, model in models.items():\n",
    "        print('>%s' % (name))\n",
    "        clf, params = model['clf'], model['params']\n",
    "        run_nested_logo(clf, params, scorer)\n",
    "\n",
    "evaluate_tuned_models(models, scorer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "# random_forest = RandomForestClassifier(n_estimators=100)\n",
    "# random_forest.fit(X_train, y_train)\n",
    "# y_pred = random_forest.predict(X_test)\n",
    "# y_pred\n",
    "# acc_random_forest = round(random_forest.score(X_test, y_test) * 100, 2)\n",
    "\n",
    "# cnt = 0\n",
    "# sitting = 0\n",
    "# standing = 0\n",
    "# for i in range(0, len(y_pred)):\n",
    "#     if not y_pred[i] == y_test[i]:\n",
    "#         cnt += 1\n",
    "#         if y_pred[i] == 1:\n",
    "#             sitting += 1\n",
    "#         else:\n",
    "#             standing += 1\n",
    "#         print('predicted', y_pred[i], 'actual', y_test[i])\n",
    "        \n",
    "# print('\\nTotal:', len(y_test))\n",
    "# print('Wrong:', cnt)\n",
    "# print('Accuracy:', acc_random_forest)\n",
    "# print('False positive:', sitting)\n",
    "# print('False negative:', standing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elizabeth/education/chair_sensor/ml_project/venv/lib/python3.5/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7017667184175723\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, balanced_accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "X_train = train_df.drop(['state', 'name'], axis=1)\n",
    "y_train = train_df['state']\n",
    "X_test = test_df.drop(['state', 'name'], axis=1)\n",
    "y_test = test_df['state']\n",
    "\n",
    "groups = train_df['name']\n",
    "\n",
    "\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Choose the type of classifier. \n",
    "clf = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "clf = GradientBoostingClassifier(n_jobs=-1)\n",
    "\n",
    "depths = [2, 4, 8, 16, 32, 64, 80, 100]\n",
    "\n",
    "# Choose some parameter combinations to try\n",
    "# parameters = {'n_estimators': [100, 200],\n",
    "#               'bootstrap': [False],\n",
    "#               'min_samples_split': [10, 20, 40, 50],\n",
    "#                'max_features': ['log2', 'sqrt','auto', None], \n",
    "#                'criterion': ['entropy', 'gini']\n",
    "# #               'max_depth': depths\n",
    "#              }\n",
    "\n",
    "parameters = {'n_estimators': [100]}\n",
    "\n",
    "# Type of scoring used to compare parameter combinations\n",
    "acc_scorer = make_scorer(balanced_accuracy_score)\n",
    "\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(clf, parameters, scoring=acc_scorer, cv=logo.split(X_train, groups=groups), n_jobs=-1)\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Set the clf to the best combination of parameters\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "# Fit the best algorithm to the data. \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predictions = clf.predict(X_test)\n",
    "print(balanced_accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False,\n",
       " 'criterion': 'gini',\n",
       " 'min_samples_split': 10,\n",
       " 'n_estimators': 200,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_obj.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=10,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_obj.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 1 accuracy: 0.7139466083621365\n",
      "Group 2 accuracy: 0.7184112208713271\n",
      "Group 3 accuracy: 0.639509015235235\n",
      "Group 4 accuracy: 0.6918121171288931\n",
      "Group 5 accuracy: 0.776493013546554\n",
      "Group 6 accuracy: 0.6587910618742846\n",
      "Group 7 accuracy: 0.7221768866070336\n",
      "Mean Accuracy: 0.7030199890893519\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "\n",
    "X_train = train_df.drop(['state', 'name'], axis=1)\n",
    "y_train = train_df['state']\n",
    "X_test = test_df.drop(['state', 'name'], axis=1)\n",
    "y_test = test_df['state']\n",
    "\n",
    "X_all = X_train.append(X_test)\n",
    "y_all = y_train.append(y_test)\n",
    "\n",
    "groups = train_df.append(test_df)['name']\n",
    "\n",
    "outcomes = []\n",
    "\n",
    "clf = RandomForestClassifier(criterion='gini',\n",
    "                             max_features=None,\n",
    "                             min_samples_split=5,\n",
    "                             n_estimators=100)\n",
    "\n",
    "#clf = GradientBoostingClassifier(n_estimators=100)\n",
    "\n",
    "def run_logo(clf):\n",
    "    logo = LeaveOneGroupOut()\n",
    "    group = 0\n",
    "    \n",
    "    for train_index, test_index in logo.split(X_all, groups=groups):\n",
    "        group += 1\n",
    "        X_train, X_test = X_all.values[train_index], X_all.values[test_index]\n",
    "        y_train, y_test = y_all.values[train_index], y_all.values[test_index]\n",
    "        clf.fit(X_train, y_train)\n",
    "        predictions = clf.predict(X_test)\n",
    "        accuracy = balanced_accuracy_score(y_test, predictions)\n",
    "        outcomes.append(accuracy)\n",
    "        print(\"Group {0} accuracy: {1}\".format(group, accuracy))\n",
    "    mean_outcome = np.mean(outcomes)\n",
    "    print(\"Mean Accuracy: {0}\".format(mean_outcome)) \n",
    "\n",
    "run_logo(clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# testing keras neural network models\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "    dataframe = read_csv(filepath, header=None)\n",
    "    return dataframe.values\n",
    "\n",
    "# load a list of files and return as a 3d numpy array\n",
    "def load_group(filenames, prefix=''):\n",
    "    loaded = list()\n",
    "    for name in filenames:\n",
    "        data = load_file(prefix + name)\n",
    "        loaded.append(data)\n",
    "    # stack group so that features are the 3rd dimension\n",
    "    print('loaded', len(loaded[0]))\n",
    "    loaded = dstack(loaded)\n",
    "    print('stacked', loaded.shape)\n",
    "    return loaded\n",
    "\n",
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group, freq, win, prefix=''):\n",
    "    filepath = prefix + group + '/'\n",
    "    # load all 9 files as a single array\n",
    "    filenames = list()\n",
    "    # total acceleration\n",
    "    filenames += ['total_acc_x_'+group+'_'+win+'_'+freq+'.csv', 'total_acc_y_'+group+'_'+win+'_'+freq+'.csv', 'total_acc_z_'+group+'_'+win+'_'+freq+'.csv']\n",
    "#     # body acceleration\n",
    "#     filenames += ['body_acc_x_'+group+'_'+win+'_'+freq+'.csv', 'body_acc_y_'+group+'_'+win+'_'+freq+'.csv', 'body_acc_z_'+group+'_'+win+'_'+freq+'.csv']\n",
    "#     # body gyroscope\n",
    "#     filenames += ['body_gyro_x_'+group+'_'+win+'_'+freq+'.csv', 'body_gyro_y_'+group+'_'+win+'_'+freq+'.csv', 'body_gyro_z_'+group+'_'+win+'_'+freq+'.csv']\n",
    "    # load input data\n",
    "    X = load_group(filenames, filepath)\n",
    "    # load class output\n",
    "    y = load_file(prefix + group + '/state_'+group+'_'+win+'_'+freq+'.csv')\n",
    "    print('X:', filenames)\n",
    "    print('y:', prefix + group + '/state_'+group+'_'+win+'_'+freq+'.csv')\n",
    "    return X, y\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(freq, win, prefix=''):\n",
    "    # load all train\n",
    "    trainX, trainy = load_dataset_group('train', freq, win, prefix)\n",
    "    print(trainX.shape, trainy.shape)\n",
    "    # load all test\n",
    "    testX, testy = load_dataset_group('test', freq, win, prefix)\n",
    "    print(testX.shape, testy.shape)\n",
    "    # zero-offset class values\n",
    "#     trainy = trainy - 1\n",
    "#     testy = testy - 1\n",
    "    # one hot encode y\n",
    "    trainy = to_categorical(trainy)\n",
    "    y_true = testy\n",
    "    testy = to_categorical(testy)\n",
    "    print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
    "    return trainX, trainy, testX, testy, y_true\n",
    "\n",
    "# summarize scores\n",
    "def summarize_results(scores):\n",
    "    print(scores)\n",
    "    m, s = mean(scores), std(scores)\n",
    "    print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 4288\n",
      "stacked (4288, 50, 3)\n",
      "X: ['total_acc_x_train_50_50.csv', 'total_acc_y_train_50_50.csv', 'total_acc_z_train_50_50.csv']\n",
      "y: ../data/processed/train/state_train_50_50.csv\n",
      "(4288, 50, 3) (4288, 1)\n",
      "loaded 1207\n",
      "stacked (1207, 50, 3)\n",
      "X: ['total_acc_x_test_50_50.csv', 'total_acc_y_test_50_50.csv', 'total_acc_z_test_50_50.csv']\n",
      "y: ../data/processed/test/state_test_50_50.csv\n",
      "(1207, 50, 3) (1207, 1)\n",
      "(4288, 50, 3) (4288, 4) (1207, 50, 3) (1207, 4)\n",
      "Train on 3430 samples, validate on 858 samples\n",
      "Epoch 1/20\n",
      "3430/3430 [==============================] - 9s 3ms/step - loss: 1.1810 - categorical_accuracy: 0.4466 - val_loss: 1.0718 - val_categorical_accuracy: 0.4767\n",
      "Epoch 2/20\n",
      "3430/3430 [==============================] - 9s 3ms/step - loss: 1.1671 - categorical_accuracy: 0.4431 - val_loss: 1.0577 - val_categorical_accuracy: 0.4767\n",
      "Epoch 3/20\n",
      "3430/3430 [==============================] - 8s 2ms/step - loss: 1.1623 - categorical_accuracy: 0.4478 - val_loss: 1.0650 - val_categorical_accuracy: 0.4767\n",
      "Epoch 4/20\n",
      "3430/3430 [==============================] - 9s 3ms/step - loss: 1.1592 - categorical_accuracy: 0.4490 - val_loss: 1.0725 - val_categorical_accuracy: 0.4767\n",
      "Epoch 5/20\n",
      "3430/3430 [==============================] - 10s 3ms/step - loss: 1.1571 - categorical_accuracy: 0.4592 - val_loss: 1.0706 - val_categorical_accuracy: 0.5618\n",
      "Epoch 6/20\n",
      "3430/3430 [==============================] - 10s 3ms/step - loss: 1.1505 - categorical_accuracy: 0.4641 - val_loss: 1.0365 - val_categorical_accuracy: 0.4499\n",
      "Epoch 7/20\n",
      "3430/3430 [==============================] - 10s 3ms/step - loss: 1.1319 - categorical_accuracy: 0.5134 - val_loss: 0.9522 - val_categorical_accuracy: 0.6352\n",
      "Epoch 8/20\n",
      "3430/3430 [==============================] - 10s 3ms/step - loss: 1.0740 - categorical_accuracy: 0.5688 - val_loss: 0.8418 - val_categorical_accuracy: 0.7145\n",
      "Epoch 9/20\n",
      "3430/3430 [==============================] - 10s 3ms/step - loss: 0.9336 - categorical_accuracy: 0.6691 - val_loss: 1.0503 - val_categorical_accuracy: 0.5862\n",
      "Epoch 10/20\n",
      "3430/3430 [==============================] - 10s 3ms/step - loss: 0.8843 - categorical_accuracy: 0.6907 - val_loss: 1.0199 - val_categorical_accuracy: 0.5839\n",
      "Epoch 11/20\n",
      "3430/3430 [==============================] - 10s 3ms/step - loss: 0.8641 - categorical_accuracy: 0.6959 - val_loss: 0.9105 - val_categorical_accuracy: 0.6270\n",
      "Epoch 12/20\n",
      "3430/3430 [==============================] - 10s 3ms/step - loss: 0.8666 - categorical_accuracy: 0.7000 - val_loss: 1.1492 - val_categorical_accuracy: 0.5839\n",
      "Epoch 13/20\n",
      "3430/3430 [==============================] - 10s 3ms/step - loss: 0.8197 - categorical_accuracy: 0.7190 - val_loss: 1.6188 - val_categorical_accuracy: 0.5781\n",
      "Epoch 14/20\n",
      "3430/3430 [==============================] - 10s 3ms/step - loss: 0.7919 - categorical_accuracy: 0.7344 - val_loss: 0.6205 - val_categorical_accuracy: 0.8170\n",
      "Epoch 15/20\n",
      "3430/3430 [==============================] - 10s 3ms/step - loss: 0.7456 - categorical_accuracy: 0.7516 - val_loss: 0.5283 - val_categorical_accuracy: 0.8590\n",
      "Epoch 16/20\n",
      "3430/3430 [==============================] - 10s 3ms/step - loss: 0.7448 - categorical_accuracy: 0.7501 - val_loss: 0.7458 - val_categorical_accuracy: 0.7354\n",
      "Epoch 17/20\n",
      "3430/3430 [==============================] - 10s 3ms/step - loss: 0.6724 - categorical_accuracy: 0.7697 - val_loss: 0.5463 - val_categorical_accuracy: 0.8030\n",
      "Epoch 18/20\n",
      "3430/3430 [==============================] - 10s 3ms/step - loss: 0.6445 - categorical_accuracy: 0.7726 - val_loss: 0.4911 - val_categorical_accuracy: 0.7902\n",
      "Epoch 19/20\n",
      "3430/3430 [==============================] - 10s 3ms/step - loss: 0.6454 - categorical_accuracy: 0.7708 - val_loss: 0.5254 - val_categorical_accuracy: 0.7902\n",
      "Epoch 20/20\n",
      "3430/3430 [==============================] - 10s 3ms/step - loss: 0.6117 - categorical_accuracy: 0.7843 - val_loss: 0.3231 - val_categorical_accuracy: 0.8800\n",
      "1207/1207 [==============================] - 1s 561us/step\n",
      "0.8624689236295648\n",
      ">#1: 86.247\n",
      "[86.24689236295649]\n",
      "Accuracy: 86.247% (+/-0.000)\n"
     ]
    }
   ],
   "source": [
    "# lstm model\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "# fit and evaluate a model\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "    verbose, epochs, batch_size = 1, 20, 10\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "    # fit network\n",
    "    model.fit(trainX, trainy, validation_split=0.2, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=1)\n",
    "    prediction = model.predict_classes(testX)\n",
    "    return prediction, accuracy, model\n",
    "\n",
    "# run an experiment\n",
    "def run_experiment(freq, win, repeats=1):\n",
    "    # load data\n",
    "    trainX, trainy, testX, testy, y_true = load_dataset(freq=freq, win=win, prefix='../data/processed/')\n",
    "#     score = evaluate_model(trainX, trainy, testX, testy)\n",
    "#     print(score)\n",
    "    # repeat experiment\n",
    "    scores = list()\n",
    "#     for r in range(repeats):\n",
    "    pred_classes, score, model = evaluate_model(trainX, trainy, testX, testy)\n",
    "    print(score)\n",
    "    score = score * 100.0\n",
    "    print('>#%d: %.3f' % (1, score))\n",
    "    scores.append(score)\n",
    "#     print(classification_report(y_true, pred_classes))\n",
    "#     print(confusion_matrix(y_true, pred_classes))\n",
    "    # summarize results\n",
    "    summarize_results(scores)\n",
    "    return pred_classes, y_true, model\n",
    "\n",
    "# run the experiment\n",
    "freq = '50'\n",
    "win = '50'\n",
    "pred_classes, y_true, model = run_experiment(freq, win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0078125, -0.015625, -1.013671875], [-0.00390625, -0.017578125, -1.02734375], [0.0087890625, -0.0078125, -1.015625], [0.0009765625, -0.009765625, -1.0234375], [0.009765625, -0.00390625, -1.0224609375], [-0.001953125, -0.01171875, -1.013671875], [-0.00390625, -0.013671875, -1.013671875], [-0.0009765625, -0.00390625, -1.0166015625], [-0.0009765625, -0.0078125, -1.013671875], [0.0078125, 0, -1.01171875], [-0.0029296875, 0, -1.0302734375], [0, 0.0029296875, -1.0078125], [0.001953125, -0.005859375, -1.009765625], [-0.0009765625, 0.00390625, -1.0205078125], [0, -0.0146484375, -1.009765625], [-0.0009765625, -0.0078125, -1.0029296875], [0.0078125, -0.005859375, -1.0283203125], [0.0029296875, -0.0107421875, -1.0107421875], [0.0029296875, -0.01171875, -1.0126953125], [-0.0009765625, -0.0126953125, -1.0068359375], [0.0048828125, -0.0087890625, -1.0166015625], [0.001953125, -0.0048828125, -1.0185546875], [0.0029296875, -0.00390625, -1.0029296875], [0.0009765625, -0.00390625, -1.015625], [-0.00390625, -0.0087890625, -1.017578125], [0.001953125, -0.013671875, -1.0166015625], [0.0087890625, -0.0078125, -1.013671875], [-0.0087890625, -0.0087890625, -1.0283203125], [0.0009765625, -0.0068359375, -1.0244140625], [-0.0029296875, -0.00390625, -1.0322265625], [0.0029296875, -0.013671875, -1.0341796875], [-0.00390625, -0.0126953125, -1.021484375], [0.005859375, -0.0068359375, -1.0283203125], [0.00390625, -0.0078125, -1.0078125], [-0.0009765625, -0.0068359375, -1.0302734375], [0.0009765625, -0.0068359375, -1.0224609375], [-0.001953125, -0.0068359375, -1.017578125], [0, -0.009765625, -1.0068359375], [-0.001953125, -0.0146484375, -1.01953125], [0.005859375, -0.0107421875, -1.0234375], [0, 0.001953125, -1.0302734375], [-0.005859375, -0.0068359375, -1.00390625], [-0.0048828125, -0.0078125, -1.025390625], [-0.0009765625, -0.00390625, -1.009765625], [0.0078125, -0.0107421875, -1.02734375], [0.001953125, -0.0146484375, -1.0078125], [0.001953125, -0.0078125, -1.0234375], [-0.001953125, -0.0087890625, -1.017578125], [0.00390625, -0.0068359375, -1.017578125], [0, -0.009765625, -1.01953125]]\n",
      "loaded 4288\n",
      "stacked (4288, 50, 3)\n",
      "X: ['total_acc_x_train_50_50.csv', 'total_acc_y_train_50_50.csv', 'total_acc_z_train_50_50.csv']\n",
      "y: ../data/processed/train/state_train_50_50.csv\n",
      "(4288, 50, 3) (4288, 1)\n",
      "loaded 1207\n",
      "stacked (1207, 50, 3)\n",
      "X: ['total_acc_x_test_50_50.csv', 'total_acc_y_test_50_50.csv', 'total_acc_z_test_50_50.csv']\n",
      "y: ../data/processed/test/state_test_50_50.csv\n",
      "(1207, 50, 3) (1207, 1)\n",
      "(4288, 50, 3) (4288, 4) (1207, 50, 3) (1207, 4)\n",
      "[[1.7189440e-01 7.6459265e-01 3.7627310e-02 2.5885688e-02]\n",
      " [2.8169519e-01 6.3698608e-01 4.4660129e-02 3.6658596e-02]\n",
      " [2.1760201e-01 7.0112598e-01 4.9808927e-02 3.1463068e-02]\n",
      " ...\n",
      " [6.0900825e-04 9.7425205e-01 1.9950500e-02 5.1884912e-03]\n",
      " [5.0121546e-04 9.7589171e-01 1.8889934e-02 4.7171237e-03]\n",
      " [5.7513476e-04 9.7393268e-01 2.0464137e-02 5.0280415e-03]]\n",
      "[[-0.0234375  -0.10253906  0.98632812]\n",
      " [-0.01855469 -0.1015625   0.99023438]\n",
      " [-0.0234375  -0.09179688  0.98828125]\n",
      " [-0.02734375 -0.10058594  0.99804688]\n",
      " [-0.01367188 -0.09082031  0.98242188]\n",
      " [-0.02929688 -0.09082031  0.9921875 ]\n",
      " [-0.02148438 -0.09863281  0.99121094]\n",
      " [-0.01953125 -0.09179688  0.98730469]\n",
      " [-0.0234375  -0.09765625  0.98925781]\n",
      " [-0.01464844 -0.09570312  0.99609375]\n",
      " [-0.0234375  -0.09667969  0.99414062]\n",
      " [-0.01367188 -0.09765625  0.984375  ]\n",
      " [-0.02148438 -0.10058594  0.97753906]\n",
      " [-0.02050781 -0.09765625  0.97460938]\n",
      " [-0.01855469 -0.10058594  0.99414062]\n",
      " [-0.02734375 -0.10449219  0.99511719]\n",
      " [-0.02441406 -0.1015625   0.98828125]\n",
      " [-0.01171875 -0.10058594  0.99316406]\n",
      " [-0.02636719 -0.09375     0.97753906]\n",
      " [-0.01464844 -0.09667969  0.99121094]\n",
      " [-0.02539062 -0.10058594  0.99609375]\n",
      " [-0.0234375  -0.10449219  0.99316406]\n",
      " [-0.03027344 -0.09960938  0.98535156]\n",
      " [-0.01953125 -0.09960938  0.98632812]\n",
      " [-0.02148438 -0.09570312  1.        ]\n",
      " [-0.02246094 -0.09960938  0.99609375]\n",
      " [-0.02539062 -0.1015625   0.98535156]\n",
      " [-0.0234375  -0.09960938  0.99414062]\n",
      " [-0.01171875 -0.1015625   0.99707031]\n",
      " [-0.03027344 -0.10546875  0.984375  ]\n",
      " [-0.01367188 -0.09765625  0.98144531]\n",
      " [-0.02246094 -0.09765625  0.97851562]\n",
      " [-0.01855469 -0.09863281  0.99121094]\n",
      " [-0.02929688 -0.09765625  0.99023438]\n",
      " [-0.03027344 -0.10253906  0.98828125]\n",
      " [-0.02246094 -0.09765625  0.98046875]\n",
      " [-0.01855469 -0.10449219  0.98632812]\n",
      " [-0.015625   -0.10253906  0.97949219]\n",
      " [-0.015625   -0.09765625  0.98730469]\n",
      " [-0.01953125 -0.09082031  0.98925781]\n",
      " [-0.02929688 -0.09277344  0.98339844]\n",
      " [-0.01660156 -0.1015625   0.97460938]\n",
      " [-0.01757812 -0.09863281  1.00097656]\n",
      " [-0.02929688 -0.10253906  0.98046875]\n",
      " [-0.01074219 -0.1015625   0.98535156]\n",
      " [-0.02734375 -0.09960938  0.99121094]\n",
      " [-0.01953125 -0.09863281  0.9921875 ]\n",
      " [-0.0234375  -0.09960938  0.99414062]\n",
      " [-0.02050781 -0.09667969  0.98046875]\n",
      " [-0.01953125 -0.09863281  0.96191406]]\n"
     ]
    }
   ],
   "source": [
    "sample = [[ [ 0.0078125, -0.015625, -1.013671875 ],[ -0.00390625, -0.017578125, -1.02734375 ],[ 0.0087890625, -0.0078125, -1.015625 ],[ 0.0009765625, -0.009765625, -1.0234375 ],  [ 0.009765625, -0.00390625, -1.0224609375 ],  [ -0.001953125, -0.01171875, -1.013671875 ],  [ -0.00390625, -0.013671875, -1.013671875 ],  [ -0.0009765625, -0.00390625, -1.0166015625 ],  [ -0.0009765625, -0.0078125, -1.013671875 ],  [ 0.0078125, 0, -1.01171875 ],  [ -0.0029296875, 0, -1.0302734375 ],  [ 0, 0.0029296875, -1.0078125 ],  [ 0.001953125, -0.005859375, -1.009765625 ],  [ -0.0009765625, 0.00390625, -1.0205078125 ],  [ 0, -0.0146484375, -1.009765625 ],  [ -0.0009765625, -0.0078125, -1.0029296875 ],  [ 0.0078125, -0.005859375, -1.0283203125 ],  [ 0.0029296875, -0.0107421875, -1.0107421875 ],  [ 0.0029296875, -0.01171875, -1.0126953125 ],  [ -0.0009765625, -0.0126953125, -1.0068359375 ],  [ 0.0048828125, -0.0087890625, -1.0166015625 ],  [ 0.001953125, -0.0048828125, -1.0185546875 ],  [ 0.0029296875, -0.00390625, -1.0029296875 ],  [ 0.0009765625, -0.00390625, -1.015625 ],  [ -0.00390625, -0.0087890625, -1.017578125 ],  [ 0.001953125, -0.013671875, -1.0166015625 ],  [ 0.0087890625, -0.0078125, -1.013671875 ],  [ -0.0087890625, -0.0087890625, -1.0283203125 ],  [ 0.0009765625, -0.0068359375, -1.0244140625 ],  [ -0.0029296875, -0.00390625, -1.0322265625 ],  [ 0.0029296875, -0.013671875, -1.0341796875 ],  [ -0.00390625, -0.0126953125, -1.021484375 ],  [ 0.005859375, -0.0068359375, -1.0283203125 ],  [ 0.00390625, -0.0078125, -1.0078125 ],  [ -0.0009765625, -0.0068359375, -1.0302734375 ],  [ 0.0009765625, -0.0068359375, -1.0224609375 ],  [ -0.001953125, -0.0068359375, -1.017578125 ],  [ 0, -0.009765625, -1.0068359375 ],  [ -0.001953125, -0.0146484375, -1.01953125 ],  [ 0.005859375, -0.0107421875, -1.0234375 ],  [ 0, 0.001953125, -1.0302734375 ],  [ -0.005859375, -0.0068359375, -1.00390625 ],  [ -0.0048828125, -0.0078125, -1.025390625 ],  [ -0.0009765625, -0.00390625, -1.009765625 ],  [ 0.0078125, -0.0107421875, -1.02734375 ],  [ 0.001953125, -0.0146484375, -1.0078125 ],  [ 0.001953125, -0.0078125, -1.0234375 ],  [ -0.001953125, -0.0087890625, -1.017578125 ],  [ 0.00390625, -0.0068359375, -1.017578125 ],  [ 0, -0.009765625, -1.01953125 ]]]\n",
    "print(sample[0])\n",
    "\n",
    "trainX, trainy, testX, testy, y_true = load_dataset(freq=freq, win=win, prefix='../data/processed/')\n",
    "probs = model.predict_proba(testX)\n",
    "\n",
    "print(probs)\n",
    "print(testX[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 24481\n",
      "stacked (24481, 50, 3)\n",
      "X: ['total_acc_x_train_50_50.csv', 'total_acc_y_train_50_50.csv', 'total_acc_z_train_50_50.csv']\n",
      "y: ../data/processed/train/state_train_50_50.csv\n",
      "(24481, 50, 3) (24481, 1)\n",
      "loaded 2985\n",
      "stacked (2985, 50, 3)\n",
      "X: ['total_acc_x_test_50_50.csv', 'total_acc_y_test_50_50.csv', 'total_acc_z_test_50_50.csv']\n",
      "y: ../data/processed/test/state_test_50_50.csv\n",
      "(2985, 50, 3) (2985, 1)\n",
      "(24481, 50, 3) (24481, 4) (2985, 50, 3) (2985, 4)\n",
      "Saved model to disk\n",
      "Loaded model from disk\n",
      "2985/2985 [==============================] - 15s 5ms/step\n",
      ">#1: 0.901\n"
     ]
    }
   ],
   "source": [
    "##### from keras.models import model_from_json\n",
    "import os\n",
    "\n",
    "trainX, trainy, testX, testy, y_true = load_dataset(freq=freq, win=win, prefix='../data/processed/')\n",
    "\n",
    "# Saving model\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "# Loading model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "#loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "# score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "# print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "\n",
    "_, accuracy = model.evaluate(testX, testy, batch_size=1, verbose=1)\n",
    "print('>#%d: %.3f' % (1, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# save model and architecture to single file\n",
    "model.save(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.3814168e-02, 3.8795219e-15, 5.5544578e-18, 9.7618586e-01]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = []\n",
    "for i in range(0,50):\n",
    "    example.append([1,0,-0.5])\n",
    "# print([example][0])\n",
    "model.predict_proba([[example]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 24481\n",
      "stacked (24481, 50, 3)\n",
      "X: ['total_acc_x_train_50_50.csv', 'total_acc_y_train_50_50.csv', 'total_acc_z_train_50_50.csv']\n",
      "y: ../data/processed/train/state_train_50_50.csv\n",
      "(24481, 50, 3) (24481, 1)\n",
      "loaded 2985\n",
      "stacked (2985, 50, 3)\n",
      "X: ['total_acc_x_test_50_50.csv', 'total_acc_y_test_50_50.csv', 'total_acc_z_test_50_50.csv']\n",
      "y: ../data/processed/test/state_test_50_50.csv\n",
      "(2985, 50, 3) (2985, 1)\n",
      "(24481, 50, 3) (24481, 4) (2985, 50, 3) (2985, 4)\n",
      "[[ 8.20312500e-02 -4.58984375e-02  9.87304688e-01]\n",
      " [ 8.59375000e-02 -5.56640625e-02  1.00976562e+00]\n",
      " [ 8.39843750e-02 -4.29687500e-02  9.99023438e-01]\n",
      " [ 8.59375000e-02 -4.49218750e-02  9.96093750e-01]\n",
      " [ 8.49609375e-02 -4.68750000e-02  1.00292969e+00]\n",
      " [ 8.69140625e-02 -3.61328125e-02  9.94140625e-01]\n",
      " [ 8.20312500e-02 -4.39453125e-02  9.91210938e-01]\n",
      " [ 9.17968750e-02 -3.71093750e-02  9.96093750e-01]\n",
      " [ 8.30078125e-02 -3.80859375e-02  1.00097656e+00]\n",
      " [ 8.98437500e-02 -5.07812500e-02  9.91210938e-01]\n",
      " [ 8.69140625e-02 -3.41796875e-02  9.98046875e-01]\n",
      " [ 9.37500000e-02 -4.00390625e-02  1.00292969e+00]\n",
      " [ 8.20312500e-02 -3.41796875e-02  1.00097656e+00]\n",
      " [ 8.59375000e-02 -3.32031250e-02  9.92187500e-01]\n",
      " [ 9.17968750e-02 -4.49218750e-02  9.89257812e-01]\n",
      " [ 8.00781250e-02 -3.51562500e-02  9.98046875e-01]\n",
      " [ 9.17968750e-02 -2.44140625e-02  1.00585938e+00]\n",
      " [ 8.69140625e-02 -2.24609375e-02  1.00976562e+00]\n",
      " [ 8.30078125e-02 -1.75781250e-02  9.84375000e-01]\n",
      " [ 7.71484375e-02 -2.83203125e-02  9.92187500e-01]\n",
      " [ 8.59375000e-02 -1.07421875e-02  9.94140625e-01]\n",
      " [ 8.39843750e-02  0.00000000e+00  9.86328125e-01]\n",
      " [ 8.49609375e-02 -1.75781250e-02  9.85351562e-01]\n",
      " [ 8.39843750e-02 -1.56250000e-02  9.84375000e-01]\n",
      " [ 8.78906250e-02 -9.76562500e-03  9.89257812e-01]\n",
      " [ 9.57031250e-02  1.95312500e-03  9.90234375e-01]\n",
      " [ 8.20312500e-02 -8.78906250e-03  9.99023438e-01]\n",
      " [ 8.88671875e-02  9.76562500e-04  9.85351562e-01]\n",
      " [ 9.57031250e-02 -2.92968750e-03  9.85351562e-01]\n",
      " [ 8.69140625e-02  4.88281250e-03  9.95117188e-01]\n",
      " [ 8.88671875e-02 -9.76562500e-04  1.00097656e+00]\n",
      " [ 8.98437500e-02 -1.95312500e-03  9.89257812e-01]\n",
      " [ 8.30078125e-02 -9.76562500e-04  9.79492188e-01]\n",
      " [ 9.47265625e-02  4.88281250e-03  1.00976562e+00]\n",
      " [ 9.08203125e-02  7.81250000e-03  9.90234375e-01]\n",
      " [ 8.69140625e-02  1.26953125e-02  1.01464844e+00]\n",
      " [ 8.30078125e-02  1.85546875e-02  9.96093750e-01]\n",
      " [ 8.88671875e-02  1.46484375e-02  9.98046875e-01]\n",
      " [ 8.69140625e-02  9.76562500e-03  9.95117188e-01]\n",
      " [ 8.10546875e-02  1.07421875e-02  9.80468750e-01]\n",
      " [ 9.08203125e-02  1.46484375e-02  9.86328125e-01]\n",
      " [ 8.69140625e-02  1.85546875e-02  1.00781250e+00]\n",
      " [ 8.39843750e-02  1.85546875e-02  9.95117188e-01]\n",
      " [ 7.22656250e-02  1.56250000e-02  9.81445312e-01]\n",
      " [ 8.59375000e-02  1.95312500e-02  9.83398438e-01]\n",
      " [ 7.71484375e-02  1.46484375e-02  9.84375000e-01]\n",
      " [ 8.59375000e-02  2.34375000e-02  9.86328125e-01]\n",
      " [ 7.91015625e-02  1.85546875e-02  9.91210938e-01]\n",
      " [ 7.81250000e-02  3.22265625e-02  1.00488281e+00]\n",
      " [ 8.20312500e-02  2.14843750e-02  1.00781250e+00]]\n"
     ]
    }
   ],
   "source": [
    "trainX, trainy, testX, testy, y_true = load_dataset(freq=freq, win=win, prefix='../data/processed/')\n",
    "print(testX[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'tensorflowjs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-686419c4aa76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save model in tfjs layer format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflowjs\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfjs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtfjs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_keras_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'tensorflowjs'"
     ]
    }
   ],
   "source": [
    "# # Save model in tfjs layer format\n",
    "# import tensorflowjs as tfjs\n",
    "\n",
    "# tfjs.converters.save_keras_model(model, \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 22952\n",
      "stacked (22952, 50, 3)\n",
      "X: ['total_acc_x_train_50_50.csv', 'total_acc_y_train_50_50.csv', 'total_acc_z_train_50_50.csv']\n",
      "y: ../data/processed/train/state_train_50_50.csv\n",
      "(22952, 50, 3) (22952, 1)\n",
      "loaded 4514\n",
      "stacked (4514, 50, 3)\n",
      "X: ['total_acc_x_test_50_50.csv', 'total_acc_y_test_50_50.csv', 'total_acc_z_test_50_50.csv']\n",
      "y: ../data/processed/test/state_test_50_50.csv\n",
      "(4514, 50, 3) (4514, 1)\n",
      "(22952, 50, 3) (22952, 4) (4514, 50, 3) (4514, 4)\n",
      "Epoch 1/15\n",
      "22952/22952 [==============================] - 44s 2ms/step - loss: 0.6862 - categorical_accuracy: 0.7197\n",
      "Epoch 2/15\n",
      "22952/22952 [==============================] - 42s 2ms/step - loss: 0.3934 - categorical_accuracy: 0.8586\n",
      "Epoch 3/15\n",
      "22952/22952 [==============================] - 42s 2ms/step - loss: 0.3613 - categorical_accuracy: 0.8688\n",
      "Epoch 4/15\n",
      "22952/22952 [==============================] - 42s 2ms/step - loss: 0.3318 - categorical_accuracy: 0.8788\n",
      "Epoch 5/15\n",
      "22952/22952 [==============================] - 42s 2ms/step - loss: 0.3141 - categorical_accuracy: 0.8867\n",
      "Epoch 6/15\n",
      "22952/22952 [==============================] - 42s 2ms/step - loss: 0.3054 - categorical_accuracy: 0.8896\n",
      "Epoch 7/15\n",
      "22952/22952 [==============================] - 42s 2ms/step - loss: 0.2918 - categorical_accuracy: 0.8946\n",
      "Epoch 8/15\n",
      "22952/22952 [==============================] - 42s 2ms/step - loss: 0.2816 - categorical_accuracy: 0.8973\n",
      "Epoch 9/15\n",
      "22952/22952 [==============================] - 42s 2ms/step - loss: 0.2674 - categorical_accuracy: 0.9041\n",
      "Epoch 10/15\n",
      "22952/22952 [==============================] - 42s 2ms/step - loss: 0.2626 - categorical_accuracy: 0.9047\n",
      "Epoch 11/15\n",
      "22952/22952 [==============================] - 42s 2ms/step - loss: 0.2635 - categorical_accuracy: 0.9058\n",
      "Epoch 12/15\n",
      "22952/22952 [==============================] - 42s 2ms/step - loss: 0.2460 - categorical_accuracy: 0.9111\n",
      "Epoch 13/15\n",
      "22952/22952 [==============================] - 42s 2ms/step - loss: 0.2412 - categorical_accuracy: 0.9122\n",
      "Epoch 14/15\n",
      "22952/22952 [==============================] - 44s 2ms/step - loss: 0.2427 - categorical_accuracy: 0.9132\n",
      "Epoch 15/15\n",
      "22952/22952 [==============================] - 43s 2ms/step - loss: 0.2319 - categorical_accuracy: 0.9146\n",
      "4514/4514 [==============================] - 2s 452us/step\n",
      ">#1: 88.901\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93      1677\n",
      "           1       0.92      0.95      0.93      1980\n",
      "           2       0.73      0.55      0.63       401\n",
      "           3       0.81      0.66      0.73       456\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      4514\n",
      "   macro avg       0.84      0.78      0.80      4514\n",
      "weighted avg       0.88      0.89      0.88      4514\n",
      "\n",
      "[[1609   24   27   17]\n",
      " [  49 1883   20   28]\n",
      " [  63   92  221   25]\n",
      " [  68   52   36  300]]\n",
      "[88.90119611946258]\n",
      "Accuracy: 88.901% (+/-0.000)\n"
     ]
    }
   ],
   "source": [
    "# cnn-lstm model\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# fit and evaluate a model\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "    # define model\n",
    "    verbose, epochs, batch_size = 1, 15, 10\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "    \n",
    "    #splitting windows into blocks because cnn learns features from the blocks,\n",
    "    #and the window timesteps have dependencies we want to learn\n",
    "    # reshape data into time steps of sub-sequences\n",
    "    n_steps, n_length = 1, 50\n",
    "    trainX = trainX.reshape((trainX.shape[0], n_steps, n_length, n_features))\n",
    "    testX = testX.reshape((testX.shape[0], n_steps, n_length, n_features))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'), input_shape=(None,n_length,n_features)))\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(Dropout(0.5)))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "    # fit network\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=1)\n",
    "    prediction = model.predict_classes(testX)\n",
    "    return prediction, accuracy\n",
    "\n",
    "# run an experiment\n",
    "def run_experiment(freq, win, repeats=1):\n",
    "    # load data\n",
    "    trainX, trainy, testX, testy, y_true = load_dataset(freq=freq, win=win, prefix='../data/processed/')\n",
    "#     score = evaluate_model(trainX, trainy, testX, testy)\n",
    "#     print(score)\n",
    "    # repeat experiment\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        pred_classes, score = evaluate_model(trainX, trainy, testX, testy)\n",
    "        score = score * 100.0\n",
    "        print('>#%d: %.3f' % (r+1, score))\n",
    "        scores.append(score)\n",
    "        print(classification_report(y_true, pred_classes))\n",
    "        print(confusion_matrix(y_true, pred_classes))\n",
    "    # summarize results\n",
    "    summarize_results(scores)\n",
    "\n",
    "# run the experiment\n",
    "freq = '50'\n",
    "win = '50'\n",
    "run_experiment(freq, win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 9182\n",
      "stacked (9182, 50, 3)\n",
      "X: ['total_acc_x_train_50_50.csv', 'total_acc_y_train_50_50.csv', 'total_acc_z_train_50_50.csv']\n",
      "y: ../data/processed/train/state_train_50_50.csv\n",
      "(9182, 50, 3) (9182, 1)\n",
      "loaded 1806\n",
      "stacked (1806, 50, 3)\n",
      "X: ['total_acc_x_test_50_50.csv', 'total_acc_y_test_50_50.csv', 'total_acc_z_test_50_50.csv']\n",
      "y: ../data/processed/test/state_test_50_50.csv\n",
      "(1806, 50, 3) (1806, 1)\n",
      "(9182, 50, 3) (9182, 4) (1806, 50, 3) (1806, 4)\n",
      "Epoch 1/15\n",
      "9182/9182 [==============================] - 11s 1ms/step - loss: 1.1147 - categorical_accuracy: 0.4720\n",
      "Epoch 2/15\n",
      "9182/9182 [==============================] - 10s 1ms/step - loss: 0.7306 - categorical_accuracy: 0.7487\n",
      "Epoch 3/15\n",
      "9182/9182 [==============================] - 10s 1ms/step - loss: 0.5211 - categorical_accuracy: 0.8093\n",
      "Epoch 4/15\n",
      "9182/9182 [==============================] - 10s 1ms/step - loss: 0.4582 - categorical_accuracy: 0.8404\n",
      "Epoch 5/15\n",
      "9182/9182 [==============================] - 10s 1ms/step - loss: 0.4269 - categorical_accuracy: 0.8561\n",
      "Epoch 6/15\n",
      "9182/9182 [==============================] - 10s 1ms/step - loss: 0.4006 - categorical_accuracy: 0.8607\n",
      "Epoch 7/15\n",
      "9182/9182 [==============================] - 10s 1ms/step - loss: 0.3790 - categorical_accuracy: 0.8684\n",
      "Epoch 8/15\n",
      "9182/9182 [==============================] - 10s 1ms/step - loss: 0.3704 - categorical_accuracy: 0.8702\n",
      "Epoch 9/15\n",
      "9182/9182 [==============================] - 10s 1ms/step - loss: 0.3565 - categorical_accuracy: 0.8780\n",
      "Epoch 10/15\n",
      "9182/9182 [==============================] - 10s 1ms/step - loss: 0.3456 - categorical_accuracy: 0.8801\n",
      "Epoch 11/15\n",
      "9182/9182 [==============================] - 10s 1ms/step - loss: 0.3338 - categorical_accuracy: 0.8795\n",
      "Epoch 12/15\n",
      "9182/9182 [==============================] - 10s 1ms/step - loss: 0.3200 - categorical_accuracy: 0.8858\n",
      "Epoch 13/15\n",
      "9182/9182 [==============================] - 10s 1ms/step - loss: 0.3162 - categorical_accuracy: 0.8870\n",
      "Epoch 14/15\n",
      "9182/9182 [==============================] - 10s 1ms/step - loss: 0.3121 - categorical_accuracy: 0.8884\n",
      "Epoch 15/15\n",
      "9182/9182 [==============================] - 10s 1ms/step - loss: 0.2940 - categorical_accuracy: 0.8977\n",
      "1806/1806 [==============================] - 1s 363us/step\n",
      ">#1: 86.268\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93       676\n",
      "           1       0.93      0.92      0.93       790\n",
      "           2       0.48      0.50      0.49       157\n",
      "           3       0.73      0.56      0.63       183\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      1806\n",
      "   macro avg       0.76      0.74      0.74      1806\n",
      "weighted avg       0.86      0.86      0.86      1806\n",
      "\n",
      "[[648   0  21   7]\n",
      " [ 34 729  25   2]\n",
      " [ 16  33  79  29]\n",
      " [ 22  18  41 102]]\n",
      "[86.26799506866523]\n",
      "Accuracy: 86.268% (+/-0.000)\n"
     ]
    }
   ],
   "source": [
    "# convlstm model\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# fit and evaluate a model\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "    # define model\n",
    "    verbose, epochs, batch_size = 1, 15, 10\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "    # reshape into subsequences (samples, time steps, rows, cols, channels)\n",
    "    n_steps, n_length = 1, 50\n",
    "    trainX = trainX.reshape((trainX.shape[0], n_steps, 1, n_length, n_features))\n",
    "    testX = testX.reshape((testX.shape[0], n_steps, 1, n_length, n_features))\n",
    "#     print('reshaped')\n",
    "#     print(trainX.shape)\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(ConvLSTM2D(filters=64, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, n_features)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "    # fit network\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=1)\n",
    "    prediction = model.predict_classes(testX)\n",
    "    return prediction, accuracy\n",
    "\n",
    "# run an experiment\n",
    "def run_experiment(freq, win, repeats=1):\n",
    "    # load data\n",
    "    trainX, trainy, testX, testy, y_true = load_dataset(freq=freq, win=win, prefix='../data/processed/')\n",
    "#     score = evaluate_model(trainX, trainy, testX, testy)\n",
    "#     print(score)\n",
    "    # repeat experiment\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        pred_classes, score = evaluate_model(trainX, trainy, testX, testy)\n",
    "        score = score * 100.0\n",
    "        print('>#%d: %.3f' % (r+1, score))\n",
    "        scores.append(score)\n",
    "        print(classification_report(y_true, pred_classes))\n",
    "        print(confusion_matrix(y_true, pred_classes))\n",
    "    # summarize results\n",
    "    summarize_results(scores)\n",
    "\n",
    "# run the experiment\n",
    "freq = '50'\n",
    "win = '50'\n",
    "run_experiment(freq, win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env.",
   "language": "python",
   "name": "tensorflow_env."
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
