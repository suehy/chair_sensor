{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, roc_auc_score, auc\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# start with linear classifiers, non-linear ones and eventually the more complex neural nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a single file as a numpy array\n",
    "# def load_file(filepath):\n",
    "#     dataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
    "#     return dataframe.values\n",
    "\n",
    "# load a dataset group, such as train or test\n",
    "# def load_dataset_group(group, prefix=''):\n",
    "#     # load input data\n",
    "#     X = load_file(prefix + group + '/X_'+group+'.txt')\n",
    "#     # load class output\n",
    "#     y = load_file(prefix + group + '/y_'+group+'.txt')\n",
    "#     return X, y\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "# def load_dataset(prefix=''):\n",
    "#     # load all train\n",
    "#     trainX, trainy = load_dataset_group('train', prefix + 'HARDataset/')\n",
    "#     print(trainX.shape, trainy.shape)\n",
    "#     # load all test\n",
    "#     testX, testy = load_dataset_group('test', prefix + 'HARDataset/')\n",
    "#     print(testX.shape, testy.shape)\n",
    "#     # flatten y\n",
    "#     trainy, testy = trainy[:,0], testy[:,0]\n",
    "#     print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
    "#     return trainX, trainy, testX, testy\n",
    "\n",
    "# def load_dataset(prefix=''):\n",
    "#     # load all train\n",
    "#     train = load_dataset_group('train', prefix + 'HARDataset/')\n",
    "#     print(trainX.shape, trainy.shape)\n",
    "#     # load all test\n",
    "#     testX, testy = load_dataset_group('test', prefix + 'HARDataset/')\n",
    "#     print(testX.shape, testy.shape)\n",
    "#     # flatten y\n",
    "#     trainy, testy = trainy[:,0], testy[:,0]\n",
    "#     print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
    "#     return trainX, trainy, testX, testy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '../data/processed/flori5hz.csv'\n",
    "# values = load_file(path)\n",
    "# #X, y = load_dataset_group()\n",
    "# values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/processed/train.csv')\n",
    "\n",
    "test_df = pd.read_csv('../data/processed/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(['state', 'name'], axis=1)\n",
    "y_train = train_df['state']\n",
    "X_test = test_df.drop(['state', 'name'], axis=1)\n",
    "y_test = test_df['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict of standard models to evaluate {name:object}\n",
    "def define_models(models=dict()):\n",
    "    # nonlinear models\n",
    "    #models['knn'] = KNeighborsClassifier()\n",
    "    models['cart'] = { 'clf': DecisionTreeClassifier(), 'params': {}}\n",
    "    #models['svm'] = SVC()\n",
    "    #models['bayes'] = GaussianNB()\n",
    "    # ensemble models\n",
    "#     models['bag'] = BaggingClassifier(n_estimators=100)\n",
    "#     models['rf'] = RandomForestClassifier(n_estimators=100)\n",
    "#     models['et'] = ExtraTreesClassifier(n_estimators=100)\n",
    "#     models['gbm'] = GradientBoostingClassifier(n_estimators=100)\n",
    "    #models['bag'] = BaggingClassifier()\n",
    "    models['rf'] = { 'clf': RandomForestClassifier(n_jobs=-1), 'params': {'n_estimators': [100, 200],\n",
    "#                                                                            'bootstrap': [False],\n",
    "#                                                                            'min_samples_split': [10, 20, 40, 50],\n",
    "#                                                                            'max_features': ['log2', 'sqrt','auto', None], \n",
    "                                                                           'criterion': ['entropy', 'gini']}}\n",
    "    #models['et'] = ExtraTreesClassifier()\n",
    "    #models['gbm'] = GradientBoostingClassifier()\n",
    "    # sgd is sensitive to feature scaling\n",
    "    #models['sgd'] = SGDClassifier()\n",
    "    #models['gp'] = GaussianProcessClassifier()\n",
    "    #models['mlp'] = MLPClassifier()\n",
    "    print('Defined %d models' % len(models))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined 2 models\n"
     ]
    }
   ],
   "source": [
    "# get model list\n",
    "models = define_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a single model\n",
    "def evaluate_model(trainX, trainy, testX, testy, model):\n",
    "    # fit the model\n",
    "    model.fit(trainX, trainy)\n",
    "    # make predictions\n",
    "    yhat = model.predict(testX)\n",
    "    # evaluate predictions\n",
    "    accuracy = balanced_accuracy_score(testy, yhat)\n",
    "    #roc_auc = roc_auc_score(testy, yhat)\n",
    "    return accuracy * 100.0\n",
    "    #return roc_auc * 100.0\n",
    "\n",
    "# evaluate a dict of models {name:object}, returns {name:score}\n",
    "def evaluate_models(trainX, trainy, testX, testy, models):\n",
    "    results = dict()\n",
    "    for name, model in models.items():\n",
    "        # evaluate the model\n",
    "        results[name] = evaluate_model(trainX, trainy, testX, testy, model['clf'])\n",
    "        # show process\n",
    "        print('>%s: %.3f' % (name, results[name]))\n",
    "    return results\n",
    "\n",
    "# print and plot the results\n",
    "def summarize_results(results, maximize=True):\n",
    "    # create a list of (name, mean(scores)) tuples\n",
    "    mean_scores = [(k,v) for k,v in results.items()]\n",
    "    # sort tuples by mean score\n",
    "    mean_scores = sorted(mean_scores, key=lambda x: x[1])\n",
    "    # reverse for descending order (e.g. for accuracy)\n",
    "    if maximize:\n",
    "        mean_scores = list(reversed(mean_scores))\n",
    "    print()\n",
    "    for name, score in mean_scores:\n",
    "        print('Name=%s, Score=%.3f' % (name, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elizabeth/education/chair_sensor/ml_project/venv/lib/python3.5/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">rf: 68.030\n"
     ]
    }
   ],
   "source": [
    "# evaluate models\n",
    "results = evaluate_models(X_train, y_train, X_test, y_test, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Name=rf, Score=68.030\n"
     ]
    }
   ],
   "source": [
    "### summarize results\n",
    "summarize_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">cart\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elizabeth/education/chair_sensor/ml_project/venv/lib/python3.5/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "Group 1 accuracy: 0.4192070445199356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elizabeth/education/chair_sensor/ml_project/venv/lib/python3.5/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "Group 2 accuracy: 0.3643525071638253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elizabeth/education/chair_sensor/ml_project/venv/lib/python3.5/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "Group 3 accuracy: 0.5458507214775296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elizabeth/education/chair_sensor/ml_project/venv/lib/python3.5/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "Group 4 accuracy: 0.5755821976884595\n",
      "Mean Accuracy: 0.47624811771243747\n",
      ">rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elizabeth/education/chair_sensor/ml_project/venv/lib/python3.5/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Group 1 accuracy: 0.5114313836482995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elizabeth/education/chair_sensor/ml_project/venv/lib/python3.5/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Group 2 accuracy: 0.36264726271324144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elizabeth/education/chair_sensor/ml_project/venv/lib/python3.5/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Group 3 accuracy: 0.6037061981238558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elizabeth/education/chair_sensor/ml_project/venv/lib/python3.5/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Group 4 accuracy: 0.6668104191823357\n",
      "Mean Accuracy: 0.5061984668146853\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import make_scorer, balanced_accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "\n",
    "all_df = train_df.append(test_df)\n",
    "all_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_all = all_df.drop(['state', 'name'], axis=1)\n",
    "y_all = all_df['state']\n",
    "\n",
    "groups = all_df['name']\n",
    "\n",
    "scorer = make_scorer(balanced_accuracy_score)\n",
    "\n",
    "outcomes = []\n",
    "\n",
    "def run_nested_logo(clf, params, scorer):\n",
    "    logo = LeaveOneGroupOut()\n",
    "    group = 0\n",
    "    \n",
    "    for train_index, test_index in logo.split(X_all, groups=groups):\n",
    "        group += 1\n",
    "        X_train, X_test = X_all.iloc[train_index], X_all.iloc[test_index]\n",
    "        y_train, y_test = y_all.iloc[train_index], y_all.iloc[test_index]\n",
    "        \n",
    "        inner_groups = all_df.iloc[all_df.index.isin(train_index)]['name']\n",
    "\n",
    "        logo = LeaveOneGroupOut()\n",
    "        grid_obj = GridSearchCV(clf, params, scoring=scorer, cv=logo.split(X_train, groups=inner_groups), n_jobs=-1)\n",
    "        grid_obj = grid_obj.fit(X_train, y_train)\n",
    "        \n",
    "        print(grid_obj.best_estimator_)\n",
    "        clf = grid_obj.best_estimator_\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        predictions = clf.predict(X_test)\n",
    "        \n",
    "        accuracy = balanced_accuracy_score(y_test, predictions)\n",
    "        outcomes.append(accuracy)\n",
    "        print(\"Group {0} accuracy: {1}\".format(group, accuracy))\n",
    "    mean_outcome = np.mean(outcomes)\n",
    "    print(\"Mean Accuracy: {0}\".format(mean_outcome))\n",
    "    \n",
    "def evaluate_tuned_models(models, scorer):\n",
    "    results = dict()\n",
    "    for name, model in models.items():\n",
    "        print('>%s' % (name))\n",
    "        clf, params = model['clf'], model['params']\n",
    "        run_nested_logo(clf, params, scorer)\n",
    "\n",
    "evaluate_tuned_models(models, scorer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "# random_forest = RandomForestClassifier(n_estimators=100)\n",
    "# random_forest.fit(X_train, y_train)\n",
    "# y_pred = random_forest.predict(X_test)\n",
    "# y_pred\n",
    "# acc_random_forest = round(random_forest.score(X_test, y_test) * 100, 2)\n",
    "\n",
    "# cnt = 0\n",
    "# sitting = 0\n",
    "# standing = 0\n",
    "# for i in range(0, len(y_pred)):\n",
    "#     if not y_pred[i] == y_test[i]:\n",
    "#         cnt += 1\n",
    "#         if y_pred[i] == 1:\n",
    "#             sitting += 1\n",
    "#         else:\n",
    "#             standing += 1\n",
    "#         print('predicted', y_pred[i], 'actual', y_test[i])\n",
    "        \n",
    "# print('\\nTotal:', len(y_test))\n",
    "# print('Wrong:', cnt)\n",
    "# print('Accuracy:', acc_random_forest)\n",
    "# print('False positive:', sitting)\n",
    "# print('False negative:', standing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elizabeth/education/chair_sensor/ml_project/venv/lib/python3.5/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7017667184175723\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, balanced_accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "X_train = train_df.drop(['state', 'name'], axis=1)\n",
    "y_train = train_df['state']\n",
    "X_test = test_df.drop(['state', 'name'], axis=1)\n",
    "y_test = test_df['state']\n",
    "\n",
    "groups = train_df['name']\n",
    "\n",
    "\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Choose the type of classifier. \n",
    "clf = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "depths = [2, 4, 8, 16, 32, 64, 80, 100]\n",
    "\n",
    "# Choose some parameter combinations to try\n",
    "parameters = {'n_estimators': [100, 200],\n",
    "              'bootstrap': [False],\n",
    "              'min_samples_split': [10, 20, 40, 50],\n",
    "               'max_features': ['log2', 'sqrt','auto', None], \n",
    "               'criterion': ['entropy', 'gini']\n",
    "#               'max_depth': depths\n",
    "             }\n",
    "\n",
    "# Type of scoring used to compare parameter combinations\n",
    "acc_scorer = make_scorer(balanced_accuracy_score)\n",
    "\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(clf, parameters, scoring=acc_scorer, cv=logo.split(X_train, groups=groups), n_jobs=-1)\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Set the clf to the best combination of parameters\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "# Fit the best algorithm to the data. \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predictions = clf.predict(X_test)\n",
    "print(balanced_accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False,\n",
       " 'criterion': 'gini',\n",
       " 'min_samples_split': 10,\n",
       " 'n_estimators': 200,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_obj.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=10,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_obj.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 1 accuracy: 0.5316714484053592\n",
      "Group 2 accuracy: 0.3710382820929218\n",
      "Group 3 accuracy: 0.600285689702276\n",
      "Group 4 accuracy: 0.7082916451037894\n",
      "Mean Accuracy: 0.5528217663260866\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "\n",
    "X_train = train_df.drop(['state', 'name'], axis=1)\n",
    "y_train = train_df['state']\n",
    "X_test = test_df.drop(['state', 'name'], axis=1)\n",
    "y_test = test_df['state']\n",
    "\n",
    "X_all = X_train.append(X_test)\n",
    "y_all = y_train.append(y_test)\n",
    "\n",
    "groups = train_df.append(test_df)['name']\n",
    "\n",
    "\n",
    "\n",
    "outcomes = []\n",
    "\n",
    "# clf = RandomForestClassifier(criterion='gini',\n",
    "#                              max_features=None,\n",
    "#                              min_samples_split=5,\n",
    "#                              n_estimators=100)\n",
    "\n",
    "def run_logo(clf):\n",
    "    logo = LeaveOneGroupOut()\n",
    "    group = 0\n",
    "    \n",
    "    for train_index, test_index in logo.split(X_all, groups=groups):\n",
    "        group += 1\n",
    "        X_train, X_test = X_all.values[train_index], X_all.values[test_index]\n",
    "        y_train, y_test = y_all.values[train_index], y_all.values[test_index]\n",
    "        clf.fit(X_train, y_train)\n",
    "        predictions = clf.predict(X_test)\n",
    "        accuracy = balanced_accuracy_score(y_test, predictions)\n",
    "        outcomes.append(accuracy)\n",
    "        print(\"Group {0} accuracy: {1}\".format(group, accuracy))\n",
    "    mean_outcome = np.mean(outcomes)\n",
    "    print(\"Mean Accuracy: {0}\".format(mean_outcome)) \n",
    "\n",
    "run_logo(clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing keras neural network models\n",
    "# lstm model\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 1160\n",
      "stacked (1160, 50, 3)\n",
      "(1160, 50, 3) (1160, 1)\n",
      "loaded 225\n",
      "stacked (225, 50, 3)\n",
      "(225, 50, 3) (225, 1)\n",
      "[[ 0]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]]\n",
      "(1160, 50, 3) (1160, 3) (225, 50, 3) (225, 3)\n",
      "0.38666666958067153\n"
     ]
    }
   ],
   "source": [
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "    dataframe = read_csv(filepath, header=None)\n",
    "    return dataframe.values\n",
    "\n",
    "# load a list of files and return as a 3d numpy array\n",
    "def load_group(filenames, prefix=''):\n",
    "    loaded = list()\n",
    "    for name in filenames:\n",
    "        data = load_file(prefix + name)\n",
    "        loaded.append(data)\n",
    "    # stack group so that features are the 3rd dimension\n",
    "    print('loaded', len(loaded[0]))\n",
    "    loaded = dstack(loaded)\n",
    "    print('stacked', loaded.shape)\n",
    "    return loaded\n",
    "\n",
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group, freq, win, prefix=''):\n",
    "    filepath = prefix + group + '/'\n",
    "    # load all 9 files as a single array\n",
    "    filenames = list()\n",
    "    # total acceleration\n",
    "    filenames += ['total_acc_x_'+group+'_'+win+'_'+freq+'.csv', 'total_acc_y_'+group+'_'+win+'_'+freq+'.csv', 'total_acc_z_'+group+'_'+win+'_'+freq+'.csv']\n",
    "#     # body acceleration\n",
    "#     filenames += ['body_acc_x_'+group+'_'+win+'_'+freq+'.csv', 'body_acc_y_'+group+'_'+win+'_'+freq+'.csv', 'body_acc_z_'+group+'_'+win+'_'+freq+'.csv']\n",
    "#     # body gyroscope\n",
    "#     filenames += ['body_gyro_x_'+group+'_'+win+'_'+freq+'.csv', 'body_gyro_y_'+group+'_'+win+'_'+freq+'.csv', 'body_gyro_z_'+group+'_'+win+'_'+freq+'.csv']\n",
    "    # load input data\n",
    "    X = load_group(filenames, filepath)\n",
    "    # load class output\n",
    "    y = load_file(prefix + group + '/state_'+group+'_'+win+'_'+freq+'.csv')\n",
    "    return X, y\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(freq, win, prefix=''):\n",
    "    # load all train\n",
    "    trainX, trainy = load_dataset_group('train', freq, win, prefix)\n",
    "    print(trainX.shape, trainy.shape)\n",
    "    # load all test\n",
    "    testX, testy = load_dataset_group('test', freq, win, prefix)\n",
    "    print(testX.shape, testy.shape)\n",
    "    # zero-offset class values\n",
    "    trainy = trainy - 1\n",
    "    testy = testy - 1\n",
    "    print(testy)\n",
    "    # one hot encode y\n",
    "    trainy = to_categorical(trainy)\n",
    "    testy = to_categorical(testy)\n",
    "    print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
    "    return trainX, trainy, testX, testy\n",
    "\n",
    "# fit and evaluate a model\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "    verbose, epochs, batch_size = 0, 15, 64\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # fit network\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "# summarize scores\n",
    "def summarize_results(scores):\n",
    "    print(scores)\n",
    "    m, s = mean(scores), std(scores)\n",
    "    print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
    "\n",
    "# run an experiment\n",
    "def run_experiment(freq, win, repeats=10):\n",
    "    # load data\n",
    "    trainX, trainy, testX, testy = load_dataset(freq=freq, win=win, prefix='../data/processed/')\n",
    "    score = evaluate_model(trainX, trainy, testX, testy)\n",
    "    print(score)\n",
    "    # repeat experiment\n",
    "#     scores = list()\n",
    "#     for r in range(repeats):\n",
    "#         score = evaluate_model(trainX, trainy, testX, testy)\n",
    "#         score = score * 100.0\n",
    "#         print('>#%d: %.3f' % (r+1, score))\n",
    "#         scores.append(score)\n",
    "#     # summarize results\n",
    "#     summarize_results(scores)\n",
    "\n",
    "# run the experiment\n",
    "freq = '50'\n",
    "win = '50'\n",
    "run_experiment(freq, win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
